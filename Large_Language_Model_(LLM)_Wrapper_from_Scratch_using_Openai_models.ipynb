{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farmanmann/WrappersWorkshop/blob/main/Large_Language_Model_(LLM)_Wrapper_from_Scratch_using_Openai_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "VBLvTBLxP32D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c61d86-e06c-46b3-d3c3-c4840f70281b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import time\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel\n",
        "import json"
      ],
      "metadata": {
        "id": "C4SmZWvcsQ9l"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"MY-API-KEY\""
      ],
      "metadata": {
        "id": "Hkzjm1gYsSI8"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def handle_error(response, retry_count, max_retry_attempts, retry_wait_time):\n",
        "    if response.status_code in {429, 500, 503} and retry_count < max_retry_attempts:\n",
        "        print(f\"Retrying after {retry_wait_time} seconds...\")\n",
        "        time.sleep(retry_wait_time)\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "QfbZLRJlsWk2"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatMessageEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, ChatMessage):\n",
        "            return obj.__dict__  # Convert ChatMessage object to a dictionary\n",
        "        elif isinstance(obj, list) and all(isinstance(item, ChatMessage) for item in obj):\n",
        "            return [item.__dict__ for item in obj]  # Convert list of ChatMessage objects to a list of dictionaries\n",
        "        return super().default(obj)\n",
        "\n",
        "class ChatModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=2000, model_name=\"gpt-3.5-turbo\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memory = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def _manage_memory(self, current_prompt_content, max_tokens):\n",
        "        total_required_token = self.max_completion_token - (count_tokens(current_prompt_content) + max_tokens)\n",
        "        combined_memory_tokens = sum(count_tokens(msg.content) for msg in self.memory)\n",
        "        removed_messages = []\n",
        "\n",
        "        while combined_memory_tokens > total_required_token:\n",
        "            removed_message = self.memory.pop(0)\n",
        "            combined_memory_tokens -= count_tokens(removed_message.content)\n",
        "            removed_messages.append(removed_message)\n",
        "\n",
        "    def _generate_prompt(self, messages, max_tokens):\n",
        "        if self.use_memory:\n",
        "            current_prompt_content = \" \".join(message.content for message in messages)\n",
        "            self._manage_memory(current_prompt_content, max_tokens)\n",
        "            final_messages = self.memory + messages\n",
        "        else:\n",
        "            final_messages = messages\n",
        "        return final_messages\n",
        "\n",
        "    def _chat_completion(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        retry_count = 0\n",
        "\n",
        "        total_tokens = sum(count_tokens(msg.content) for msg in messages)\n",
        "        if total_tokens + max_tokens > self.max_completion_token:\n",
        "            return \"Error: Total tokens exceed the limit.\"\n",
        "\n",
        "        while retry_count < kwargs.get(\"max_retry_attempts\", 3):\n",
        "            try:\n",
        "                json_string = json.dumps(messages, cls=ChatMessageEncoder)\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=kwargs.get(\"model\", self.model_name),\n",
        "                    messages=json.loads(json_string),  # Load JSON string as a list\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, kwargs.get(\"max_retry_attempts\", 3), kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _chat_completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        if len(messages) == 0:\n",
        "            return \"Error: No input messages.\"\n",
        "\n",
        "        prompt = self._generate_prompt(messages, max_tokens)\n",
        "\n",
        "        response = self._chat_completion(prompt, **kwargs)\n",
        "\n",
        "        try:\n",
        "            if response and response.choices and len(response.choices) > 0:\n",
        "                if self.use_memory:\n",
        "                    self.memory.extend(messages)\n",
        "                    response_dict = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response.choices[0].message.content\n",
        "                    }\n",
        "                    self.memory.append(response_dict)\n",
        "                return response\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def prioritize_messages(self, messages: List[ChatMessage]):\n",
        "        # Sort messages based on timestamp or relevance score\n",
        "        # Ensure that higher-priority messages come first in the list\n",
        "        sorted_messages = sorted(messages, key=lambda msg: msg.timestamp, reverse=True)\n",
        "        return sorted_messages\n",
        "\n",
        "    def split_long_conversation(self, messages: List[ChatMessage], max_tokens_per_chunk):\n",
        "        split_chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "\n",
        "        for msg in messages:\n",
        "            msg_tokens = count_tokens(msg.content)\n",
        "            if current_chunk_tokens + msg_tokens <= max_tokens_per_chunk:\n",
        "                current_chunk.append(msg)\n",
        "                current_chunk_tokens += msg_tokens\n",
        "            else:\n",
        "                split_chunks.append(current_chunk)\n",
        "                current_chunk = [msg]\n",
        "                current_chunk_tokens = msg_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            split_chunks.append(current_chunk)\n",
        "\n",
        "        return split_chunks"
      ],
      "metadata": {
        "id": "42rBoxf5o8Ew"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion Model Wrapper\n",
        "class CompletionModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=3000, model_name=\"gpt-3.5-turbo-instruct\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memories = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.completion_model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "        self.max_retry_attempts = 3\n",
        "\n",
        "    def _manage_memory(self, current_prompt, max_tokens):\n",
        "        number_of_token_in_current_prompt = count_tokens(current_prompt, model_name=self.completion_model_name)\n",
        "        total_memory_tokens = sum(count_tokens(memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(memory[\"AI\"], model_name=self.completion_model_name) for memory in self.memories)\n",
        "\n",
        "        while total_memory_tokens > self.max_completion_token - (number_of_token_in_current_prompt + max_tokens):\n",
        "            removed_memory = self.memories.pop(0)\n",
        "            total_memory_tokens -= count_tokens(removed_memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(removed_memory[\"AI\"], model_name=self.completion_model_name)\n",
        "\n",
        "    def _format_conversation(self, current_prompt):\n",
        "        if self.use_memory:\n",
        "            conversation_series = \"\\n\".join([f\"User: {memory['USER']}\\nAI: {memory['AI']}\" for memory in self.memories])\n",
        "            conversation_series += f\"\\nUser: {current_prompt}\\nAI:\"\n",
        "            return conversation_series\n",
        "        else:\n",
        "            return current_prompt\n",
        "\n",
        "    def _completion(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        prompt_with_memory = self._format_conversation(prompt)\n",
        "        retry_count = 0\n",
        "        while retry_count < self.max_retry_attempts:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    model=self.completion_model_name,\n",
        "                    prompt=prompt_with_memory,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,  # Control randomness of output\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e.response, retry_count, self.max_retry_attempts, kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        res = self._completion(prompt, max_tokens, temperature, **kwargs)\n",
        "        if res:\n",
        "            memory = {\n",
        "                \"USER\": prompt,\n",
        "                \"AI\": res.choices[0].text.strip()\n",
        "            }\n",
        "            self.memories.append(memory)\n",
        "            self._manage_memory(prompt, max_tokens)  # Dynamic memory management\n",
        "        return res\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory"
      ],
      "metadata": {
        "id": "dlIWrPSgtdtV"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Wrapper\n",
        "class LLMWrapper:\n",
        "    def __init__(self, api_key, model_type, use_memory=True, max_chat_completion_token=3000, model_name=\"gpt-3.5-turbo\", completion_model_name=\"gpt-3.5-turbo-instruct\"):\n",
        "        self.api_key = api_key\n",
        "        self.model_type = model_type\n",
        "        self.use_memory = use_memory\n",
        "        self.max_chat_completion_token = max_chat_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.completion_model_name = completion_model_name\n",
        "        self.chat_wrapper = ChatModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "        self.completion_wrapper = CompletionModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.completion_model_name)\n",
        "\n",
        "    def generate_response(self, messages_or_prompt, max_tokens: int = 2000, **kwargs) -> Union[openai.ChatCompletion, openai.Completion, str]:\n",
        "        if isinstance(messages_or_prompt, str):\n",
        "            prompt = messages_or_prompt\n",
        "            messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        else:\n",
        "            messages = messages_or_prompt\n",
        "            prompt = messages[-1].content\n",
        "\n",
        "        if self.model_type == \"Chat\":\n",
        "            res = self.chat_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].message\n",
        "        elif self.model_type == \"Completion\":\n",
        "            res = self.completion_wrapper.generate_response(prompt, max_tokens, **kwargs)\n",
        "            return res.choices[0].text.strip()\n",
        "        else:\n",
        "            return \"Invalid model_type specified.\""
      ],
      "metadata": {
        "id": "bTZZvVWgtfMV"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Chat\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello, how are you?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I'm doing well, thank you!\")\n",
        "]\n",
        "\n",
        "response = llm_wrapper.generate_response(messages, max_tokens=100)\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBSbwIPz4HF",
        "outputId": "5659fa49-8c6a-40fe-8899-5cf3555fe0a9"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"How can I assist you today?\",\n",
            "  \"refusal\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLMWrapper with the model type \"Completion\"\n",
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Completion\")\n",
        "\n",
        "# Define the prompt for generating completion\n",
        "prompt = \"Once upon a time in a land far, far away\"\n",
        "\n",
        "# Generate a response using the provided prompt\n",
        "response = llm_wrapper.generate_response(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTSOfrS248GS",
        "outputId": "9797878e-ea74-4bfa-dbf4-cc9710ac60e8"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: there was a beautiful kingdom ruled by a benevolent king and queen. The kingdom was filled with lush green forests, crystal clear lakes, and majestic mountains. The people of the kingdom were happy and content, living in harmony with each other and nature.\n",
            "\n",
            "One day, a dark cloud appeared in the sky and threatened to destroy the peace and happiness of the kingdom. A wicked sorcerer had cast a spell on the land, causing all the crops to wither and animals to fall ill. The people were afraid and did not know what to do.\n",
            "\n",
            "The king and queen sought the help of a powerful fairy who lived in the deep forests. She told them that the only way to break the spell was to find a rare flower that only bloomed in the top of the tallest mountain.\n",
            "\n",
            "The king called upon his bravest knights and set off on a journey to find the flower. They encountered many challenges along the way, but they persevered, driven by their love for their kingdom and its people.\n",
            "\n",
            "After many days of traveling, they finally reached the top of the mountain and found the flower. They brought it back to the kingdom and the queen used her magic to turn it into a powerful potion. She sprinkled the potion all over the kingdom, breaking the spell and restoring the land to its former beauty.\n",
            "\n",
            "The people rejoiced and the king and queen were hailed as heroes. The wicked sorcerer was vanquished and peace was restored to the land.\n",
            "\n",
            "From that day on, the kingdom prospered and the people lived happily ever after. And the rare flower, now known as the \"flower of hope,\" bloomed every year as a reminder of the bravery and courage of the king and his knights.\n"
          ]
        }
      ]
    }
  ]
}