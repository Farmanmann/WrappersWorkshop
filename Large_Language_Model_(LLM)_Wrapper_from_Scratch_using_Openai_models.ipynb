{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farmanmann/WrappersWorkshop/blob/main/Large_Language_Model_(LLM)_Wrapper_from_Scratch_using_Openai_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "VBLvTBLxP32D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803e514b-73e0-4f42-b3b1-fb8db133bece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import time\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel\n",
        "import json"
      ],
      "metadata": {
        "id": "C4SmZWvcsQ9l"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"\""
      ],
      "metadata": {
        "id": "Hkzjm1gYsSI8"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def handle_error(response, retry_count, max_retry_attempts, retry_wait_time):\n",
        "    if response.status_code in {429, 500, 503} and retry_count < max_retry_attempts:\n",
        "        print(f\"Retrying after {retry_wait_time} seconds...\")\n",
        "        time.sleep(retry_wait_time)\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "QfbZLRJlsWk2"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatMessageEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, ChatMessage):\n",
        "            return obj.__dict__  # Convert ChatMessage object to a dictionary\n",
        "        elif isinstance(obj, list) and all(isinstance(item, ChatMessage) for item in obj):\n",
        "            return [item.__dict__ for item in obj]  # Convert list of ChatMessage objects to a list of dictionaries\n",
        "        return super().default(obj)\n",
        "\n",
        "class ChatModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=2000, model_name=\"gpt-3.5-turbo\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memory = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def _manage_memory(self, current_prompt_content, max_tokens):\n",
        "        total_required_token = self.max_completion_token - (count_tokens(current_prompt_content) + max_tokens)\n",
        "        combined_memory_tokens = sum(count_tokens(msg.content) for msg in self.memory)\n",
        "        removed_messages = []\n",
        "\n",
        "        while combined_memory_tokens > total_required_token:\n",
        "            removed_message = self.memory.pop(0)\n",
        "            combined_memory_tokens -= count_tokens(removed_message.content)\n",
        "            removed_messages.append(removed_message)\n",
        "\n",
        "    def _generate_prompt(self, messages, max_tokens):\n",
        "        if self.use_memory:\n",
        "            current_prompt_content = \" \".join(message.content for message in messages)\n",
        "            self._manage_memory(current_prompt_content, max_tokens)\n",
        "            final_messages = self.memory + messages\n",
        "        else:\n",
        "            final_messages = messages\n",
        "        return final_messages\n",
        "\n",
        "    def _chat_completion(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        retry_count = 0\n",
        "\n",
        "        total_tokens = sum(count_tokens(msg.content) for msg in messages)\n",
        "        if total_tokens + max_tokens > self.max_completion_token:\n",
        "            return \"Error: Total tokens exceed the limit.\"\n",
        "\n",
        "        while retry_count < kwargs.get(\"max_retry_attempts\", 3):\n",
        "            try:\n",
        "                json_string = json.dumps(messages, cls=ChatMessageEncoder)\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=kwargs.get(\"model\", self.model_name),\n",
        "                    messages=json.loads(json_string),  # Load JSON string as a list\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, kwargs.get(\"max_retry_attempts\", 3), kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _chat_completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        if len(messages) == 0:\n",
        "            return \"Error: No input messages.\"\n",
        "\n",
        "        prompt = self._generate_prompt(messages, max_tokens)\n",
        "\n",
        "        response = self._chat_completion(prompt, **kwargs)\n",
        "\n",
        "        try:\n",
        "            if response and response.choices and len(response.choices) > 0:\n",
        "                if self.use_memory:\n",
        "                    self.memory.extend(messages)\n",
        "                    response_dict = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response.choices[0].message.content\n",
        "                    }\n",
        "                    self.memory.append(response_dict)\n",
        "                return response\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def prioritize_messages(self, messages: List[ChatMessage]):\n",
        "        # Sort messages based on timestamp or relevance score\n",
        "        # Ensure that higher-priority messages come first in the list\n",
        "        sorted_messages = sorted(messages, key=lambda msg: msg.timestamp, reverse=True)\n",
        "        return sorted_messages\n",
        "\n",
        "    def split_long_conversation(self, messages: List[ChatMessage], max_tokens_per_chunk):\n",
        "        split_chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "\n",
        "        for msg in messages:\n",
        "            msg_tokens = count_tokens(msg.content)\n",
        "            if current_chunk_tokens + msg_tokens <= max_tokens_per_chunk:\n",
        "                current_chunk.append(msg)\n",
        "                current_chunk_tokens += msg_tokens\n",
        "            else:\n",
        "                split_chunks.append(current_chunk)\n",
        "                current_chunk = [msg]\n",
        "                current_chunk_tokens = msg_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            split_chunks.append(current_chunk)\n",
        "\n",
        "        return split_chunks"
      ],
      "metadata": {
        "id": "42rBoxf5o8Ew"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion Model Wrapper\n",
        "class CompletionModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=3000, model_name=\"gpt-3.5-turbo-instruct\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memories = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.completion_model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "        self.max_retry_attempts = 3\n",
        "\n",
        "    def _manage_memory(self, current_prompt, max_tokens):\n",
        "        number_of_token_in_current_prompt = count_tokens(current_prompt, model_name=self.completion_model_name)\n",
        "        total_memory_tokens = sum(count_tokens(memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(memory[\"AI\"], model_name=self.completion_model_name) for memory in self.memories)\n",
        "\n",
        "        while total_memory_tokens > self.max_completion_token - (number_of_token_in_current_prompt + max_tokens):\n",
        "            removed_memory = self.memories.pop(0)\n",
        "            total_memory_tokens -= count_tokens(removed_memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(removed_memory[\"AI\"], model_name=self.completion_model_name)\n",
        "\n",
        "    def _format_conversation(self, current_prompt):\n",
        "        if self.use_memory:\n",
        "            conversation_series = \"\\n\".join([f\"User: {memory['USER']}\\nAI: {memory['AI']}\" for memory in self.memories])\n",
        "            conversation_series += f\"\\nUser: {current_prompt}\\nAI:\"\n",
        "            return conversation_series\n",
        "        else:\n",
        "            return current_prompt\n",
        "\n",
        "    def _completion(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        prompt_with_memory = self._format_conversation(prompt)\n",
        "        retry_count = 0\n",
        "        while retry_count < self.max_retry_attempts:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    model=self.completion_model_name,\n",
        "                    prompt=prompt_with_memory,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,  # Control randomness of output\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e.response, retry_count, self.max_retry_attempts, kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        res = self._completion(prompt, max_tokens, temperature, **kwargs)\n",
        "        if res:\n",
        "            memory = {\n",
        "                \"USER\": prompt,\n",
        "                \"AI\": res.choices[0].text.strip()\n",
        "            }\n",
        "            self.memories.append(memory)\n",
        "            self._manage_memory(prompt, max_tokens)  # Dynamic memory management\n",
        "        return res\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory"
      ],
      "metadata": {
        "id": "dlIWrPSgtdtV"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Wrapper\n",
        "class LLMWrapper:\n",
        "    def __init__(self, api_key, model_type, use_memory=True, max_chat_completion_token=3000, model_name=\"gpt-3.5-turbo\", completion_model_name=\"gpt-3.5-turbo-instruct\"):\n",
        "        self.api_key = api_key\n",
        "        self.model_type = model_type\n",
        "        self.use_memory = use_memory\n",
        "        self.max_chat_completion_token = max_chat_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.completion_model_name = completion_model_name\n",
        "        self.chat_wrapper = ChatModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "        self.completion_wrapper = CompletionModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.completion_model_name)\n",
        "\n",
        "    def generate_response(self, messages_or_prompt, max_tokens: int = 2000, **kwargs) -> Union[openai.ChatCompletion, openai.Completion, str]:\n",
        "        if isinstance(messages_or_prompt, str):\n",
        "            prompt = messages_or_prompt\n",
        "            messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        else:\n",
        "            messages = messages_or_prompt\n",
        "            prompt = messages[-1].content\n",
        "\n",
        "        if self.model_type == \"Chat\":\n",
        "            res = self.chat_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].message\n",
        "        elif self.model_type == \"Completion\":\n",
        "            res = self.completion_wrapper.generate_response(prompt, max_tokens, **kwargs)\n",
        "            return res.choices[0].text.strip()\n",
        "        else:\n",
        "            return \"Invalid model_type specified.\""
      ],
      "metadata": {
        "id": "bTZZvVWgtfMV"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Chat\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello, how are you?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I'm doing well, thank you!\")\n",
        "]\n",
        "\n",
        "response = llm_wrapper.generate_response(messages, max_tokens=100)\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBSbwIPz4HF",
        "outputId": "5aa6d56f-6057-4323-c327-9fe0a6766eb7"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"How can I assist you today?\",\n",
            "  \"refusal\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLMWrapper with the model type \"Completion\"\n",
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Completion\")\n",
        "\n",
        "# Define the prompt for generating completion\n",
        "prompt = \"Once upon a time in a land far, far away\"\n",
        "\n",
        "# Generate a response using the provided prompt\n",
        "response = llm_wrapper.generate_response(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTSOfrS248GS",
        "outputId": "66464608-8fad-4768-e818-062577a35a35"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: There was a small kingdom ruled by a wise king and queen. The kingdom was surrounded by lush greenery and beautiful fields of flowers. The people of the kingdom lived in peace and harmony, and they were grateful to their rulers for their prosperous and happy lives.\n",
            "\n",
            "The king and queen had a daughter, Princess Lily, who was known for her kind heart and adventurous spirit. She loved exploring the kingdom and often went on adventures with her loyal horse, Breeze.\n",
            "\n",
            "One day, while on a ride with Breeze, Princess Lily stumbled upon a hidden cave. Curious and intrigued, she ventured inside and found a magical book. As she opened the book, a bright light filled the cave, and the pages of the book started to glow.\n",
            "\n",
            "Princess Lily couldn't believe her eyes as she read the words on the pages. The book was filled with stories and creatures from distant lands, and it had the power to transport her to any place she wished.\n",
            "\n",
            "Excited and full of wonder, Princess Lily quickly closed the book and brought it back to the castle. She showed it to her parents, who were both amazed and overjoyed at the discovery.\n",
            "\n",
            "From that day on, Princess Lily and her horse Breeze embarked on countless adventures, traveling to far-off lands and meeting all kinds of magical creatures. And the people of the kingdom lived happily ever after, knowing that their princess was always there to protect and bring wonder to their lives.\n"
          ]
        }
      ]
    }
  ]
}